<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Zoë Wilkinson Saldaña</title>
    <description>Data librarian &amp; developer</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Mon, 14 Jan 2019 10:24:49 -0500</pubDate>
    <lastBuildDate>Mon, 14 Jan 2019 10:24:49 -0500</lastBuildDate>
    <generator>Jekyll v3.6.2</generator>
    
      <item>
        <title>Stories from the city, stories from the cloud: an introduction to city open data portals in the United States</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://medium.com/messy-data/stories-from-the-city-stories-from-the-cloud-an-introduction-to-city-open-data-portals-in-the-636ab60500bc&quot;&gt;&lt;img src=&quot;http://localhost:4000/img/stories_from_city.png&quot; alt=&quot;bar chart showing number of datasets in open data portal for cities with top fifteen highest Census ratings.&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;What stories can you tell with a city’s open data? And what’s missing from the data?…&lt;/p&gt;

  &lt;p&gt;In this post, I will introduce you to the producers and gatekeepers of civic data that Google describes as they play out in practice. When it comes to cities, these questions tend to revolve around the city open data portal: a model adopted nation-wide that facilitates tens of thousands of datasets, a buzzing cross-section of hyper-local civic data activity, and a service almost exclusively built on just one tech company’s platform.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;https://medium.com/messy-data/stories-from-the-city-stories-from-the-cloud-an-introduction-to-city-open-data-portals-in-the-636ab60500bc&quot;&gt;Read on Messy Data (my new feminist critical data science blog!)&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 10 Jan 2019 12:22:00 -0500</pubDate>
        <link>http://localhost:4000/articles/2019-01/city-open-data</link>
        <guid isPermaLink="true">http://localhost:4000/articles/2019-01/city-open-data</guid>
        
        
        <category>essay</category>
        
      </item>
    
      <item>
        <title>Creating a custom network visualization using the Scalar API Explorer, Part 1</title>
        <description>&lt;p&gt;&lt;a href=&quot;https://scalar.me/anvc/scalar/&quot;&gt;Scalar&lt;/a&gt; is a unique and powerful open source publishing platform. Its strength lies in its ability to combine linear and nonlinear methods of exploring media, narratives, annotations, and scholarship within the single organizing structure of a &lt;strong&gt;book&lt;/strong&gt;. Scalar also provides &lt;a href=&quot;http://scalar.usc.edu/works/guide2/visualizations?path=index&quot;&gt;several out-of-the-box options&lt;/a&gt; to visualize your data - including as an interactive network visualization.&lt;/p&gt;

&lt;p&gt;But what happens when you want to customize your visualizations beyond what the Scalar presets allow for? I recently ran up against this issue and decided to find a way to create a network visualization “from scratch” (in reality, leveraging a number of excellent open source tools, demos, and APIs!) I used the &lt;a href=&quot;http://scalar.usc.edu/tools/apiexplorer/&quot;&gt;Scalar API Explorer&lt;/a&gt; to export data about our pages and tags between pages, prepared the network data with Python and various packages (NetworkX, BeautifulSoup, etc.), and wrote a custom network visualization using D3.js and Canvas.&lt;/p&gt;

&lt;p&gt;I waded through a fair bit of code and experimentation along the way, and I’d like to share with you some notes, lessons, code, and tools that reuslted from that process. I also tried to identify several places where you may wish to deviate from my process, or to experiment further, depending on your Scalar book and your own vision of what such a visualization might look like.&lt;/p&gt;

&lt;p&gt;This tutorial is written as two parts:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Part 1&lt;/strong&gt;: Represent your Scalar book as a network using Python and the Scalar API Explorer&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Part 2&lt;/strong&gt;: Create an interactive visualization of the network data using D3.js and Canvas&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In Part 1, I will introduce the goals of this process and walk through the Python code needed to prepare your data.&lt;/p&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;h3 id=&quot;why-a-custom-visualization&quot;&gt;Why a custom visualization?&lt;/h3&gt;

&lt;p&gt;Here is an example of a &lt;a href=&quot;http://scalar.usc.edu/works/jewish-cafes/network-visualization-of-caf-regulars&quot;&gt;built-in network visualization generated in Scalar&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;And here is that same book reimagined as a &lt;a href=&quot;https://zoews.github.io/interactive-network-viz/viz2.html&quot;&gt;custom visualization using D3.js and Canvas&lt;/a&gt; – &lt;em&gt;note that as of 8/21/18 this visualization is still a work in progress, with more photography/text and reorganized tagging to come soon.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;If you want to use a pre-built network visualization in Scalar, the process is fairly simple: create a new page, and assign that page the &lt;strong&gt;Connections&lt;/strong&gt; layout style. Now when you visit that page, Scalar will fetch all pages and tags between pages (which are expressed as “x is a tag of y” and “x tags y” in the &lt;strong&gt;Relationships&lt;/strong&gt; tag of a page). Scalar will then use the JavaScript visualization D3.js to automatically generate a &lt;a href=&quot;https://en.wikipedia.org/wiki/Force-directed_graph_drawing&quot;&gt;force directed&lt;/a&gt; network visualization.&lt;/p&gt;

&lt;p&gt;For our purposes, we encountered a number of limitations with this format. First, the network visualization takes considerable time to load a book with hundreds of items (up to 30 seconds). Behind the scenes, Scalar is assembling the data by fetching it in batches of 25 pages/tags/media items at a time, and stopping to rest between each request. This pattern of request-wait-request-wait results in a slowed-down experience for the user.&lt;/p&gt;

&lt;p&gt;Second, the prebuilt options do not provide any options for customizing the final output. We wanted to create a network visualization that (1) excluded any “orphan” pages (pages that lack any tag relationships), (2) includes a more streamlined visual style that matches other aspects of our project, (3) populates a sidebar with names/descriptions/images from the Scalar book, and (4) allows the user to take the additional step of following a link directly to the Scalar page in focus.&lt;/p&gt;

&lt;h3 id=&quot;our-goal-is-to-prepare-data-for-a-network-visualization&quot;&gt;Our goal is to prepare data for a network visualization&lt;/h3&gt;

&lt;p&gt;While your ideal visualization may differ from ours, the basic process for preparing the data will be the same:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Step 1: Download your Scalar book data using the Scalar API Explorer&lt;/li&gt;
  &lt;li&gt;Step 2: Restructure the raw Scalar output as network data&lt;/li&gt;
  &lt;li&gt;Step 3: Add extra network analysis indicators to the visualization, and&lt;/li&gt;
  &lt;li&gt;Step 4: Gather and export data as single .json file&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When deciding how to prepare data for a visualization, it’s helpful to have a vision of what the final output will look like. In this case, I used &lt;a href=&quot;https://bl.ocks.org/mbostock/ad70335eeef6d167bc36fd3c04378048&quot;&gt;this excellent code sample from Mike Bostock&lt;/a&gt; as a starting point to my design. I noticed that this network visualization relies on a single JSON file called “miserables.json” that consists of a collection of “nodes” and a collection of “links”. Finding this code sample allowed me to set a goal for my data preparation: &lt;strong&gt;represent all tags and pages as links and nodes within a single .json file&lt;/strong&gt;.&lt;/p&gt;

&lt;h3 id=&quot;wait-whats-a-link-whats-a-node&quot;&gt;Wait, what’s a link!? What’s a node!?&lt;/h3&gt;

&lt;p&gt;Okay, I realize the above paragraph may have sound quite strange if you haven’t been mucking around network visualization design lately! Let me explain what’s happening here.&lt;/p&gt;

&lt;p&gt;A network visualization is a way to represent the relationships between many different individuals/items/entities all at once. It provides your user a summary view of the overall structure of a community (is it made up of totally disconnected cliques? Does everyone know one super-popular person?) Network visualizations often provide methods for exploring and investigating these connections, such as click-and-dragging, zooming, etc. Also, they tend to be &lt;a href=&quot;https://flowingdata.com/category/visualization/network-visualization/&quot;&gt;quite pretty and compelling&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;One way to think of a network is a set of points or “nodes” (which could be anything from people to countries to books to cities) and a set of relationships between those nodes, which are commonly called links or (less intuitively) “edges.”&lt;/p&gt;

&lt;p&gt;You can also make things more complicated: sometimes the relationships between nodes are one-way (Alma sends letters to Julie, but Julie doesn’t write Alma back) or unidirectional, and other times they are two-way or bidirectional. Also, some relationships might be more important than others, which is sometimes captured as the “weight” of an edge/link.&lt;/p&gt;

&lt;p&gt;For our purposes, we are focused on the “nodes” and “links”. In the context of a Scalar book, you can think of the nodes the same way Scalar thinks of them: pages and media items. In our team’s example, these pages refer to individuals, cities, cafes, photographs, drawings, etc.&lt;/p&gt;

&lt;p&gt;We can also think about “links” in terms of tags. That is, if a page tags another page, we can think of a link existing between those two pages. In Scalar, tags are unidirectional (page A tags page B, or conversely, page A is tagged by page B), though two tags can exist back and forth between two pages, thus creating a bidirectional relationship.&lt;/p&gt;

&lt;p&gt;Our goal will be to capture information about the “nodes” of our network (in our case, pages of specific kinds) and “links” of our network (in our case, tags recorded in the Scalar book). If you’ve already created a Scalar book with at least two pages and at least one tag, then congratulations! You have enough data to generate a network visualization.&lt;/p&gt;

&lt;h2 id=&quot;step-1-download-your-scalar-book-data-using-the-scalar-api-explorer&quot;&gt;Step 1: Download your Scalar book data using the Scalar API Explorer&lt;/h2&gt;

&lt;p&gt;The Scalar team has done an excellent job anticipating that some users (like you!) may wish to move beyond the presets and limitations of the out-of-the-box platform features. So to facilitate that process, they created an easy mechanism for dumping out some or all of the data from your Scalar book.&lt;/p&gt;

&lt;p&gt;The main tool to accomplish this task, for us, will be the &lt;a href=&quot;http://scalar.usc.edu/tools/apiexplorer/&quot;&gt;Scalar API Explorer&lt;/a&gt;. This is a fun tool to explore in general. For today, we’re going to follow a targeted approach:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Follow the first direction under “Query” and copy and paste the URL from your book into the text box. I generally use the Welcome page in my Scalar book, but I believe this should work no matter which page you choose.&lt;/li&gt;
  &lt;li&gt;In the options for what to export, select the final one: “All of the book’s content and relationships (potentially slow, use with care)”&lt;/li&gt;
  &lt;li&gt;Don’t change any of the other options. You should be ignoring past versions and (most importantly) exporting as RDF-JSON.&lt;/li&gt;
  &lt;li&gt;Click the big “Get API results” button. If this doesn’t seem to work, try refreshing, copy and pasting a different URL, etc. You will know the API is fetching your data when the text box below fills with “Retrieving content…” and it may take a minute or longer to execute.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here’s what the results look like:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/img/scalar-network/api_output.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The Scalar API Explorer will also generate some results in the “Text” field below, but we’re going to ignore this second field and focus entirely on the output in the Data text box (the Text is also included within the Data output, it’s just harder to single out right now.)&lt;/p&gt;

&lt;p&gt;To use this output, we must download it locally to our computer. Copy the entire contents of the Data text field to the clipboard. Next, open up your text editor of choice (I use Visual Studio Code for these sorts of tasks, but you may prefer Sublime Text, or &lt;a href=&quot;https://github.com/collections/text-editors&quot;&gt;another simple text editor&lt;/a&gt;) and paste the output into a blank file. Finally, save this file with .json extension to indicate it is a JSON file. I suggest the filename &lt;strong&gt;scalar_output.json&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.json.org/&quot;&gt;JSON&lt;/a&gt; is a format desgined to easily move collections of data between programs. For a JSON file to be read by a Python or JavaScript program, it must follow certain syntax conventions. However the formatting itself is plain text, and thus is interchangeable with a plain text (.txt) file. It becomes a .json file simply when given the .json extension.&lt;/p&gt;

&lt;h2 id=&quot;step-2-restructure-the-raw-scalar-output-as-network-data&quot;&gt;Step 2: Restructure the raw Scalar output as network data&lt;/h2&gt;

&lt;p&gt;Now that we have our raw data, we must re-structure it a way that will be useful for our network visualization later on.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(Here’s where you may wish to jump straight to one solution - a ready-to-run Python script called &lt;a href=&quot;https://github.com/zoews/interactive-network-viz/blob/master/parsing-tools/generate_scalar_network.py&quot;&gt;generate_scalar_network.py&lt;/a&gt;. To download directly from GitHub, click the “Raw” button on the preview page, and save the resulting page to your computer as a Python file. Place your &lt;strong&gt;scalar_output.json&lt;/strong&gt; file in the same directory, and run the script in the terminal. However, I recommend you go through the step-by-step breakdown below. It’s likely that you will wish to customize some of the behaviors of the script in order to capture or omit elements of your Scalar book, in whatever way is most meaningful for your end goal. Plus you may wish to take the functions we describe below and further reimagine them, add new functions, etc. Feel free to choose your own adventure!)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Python is an excellent tool for this task! Open up your text editor of choice again and save a new file as &lt;strong&gt;generate_scalar_network.py&lt;/strong&gt; . We will be using Python 3 for our code examples here. If you need a refresher on using Python 3, such as on using pip to install Python libraries (you’ll need to install the libraries below) or running Python programs from the command line, you may wish to explore &lt;a href=&quot;https://www.python.org/about/gettingstarted/&quot;&gt;a few resources&lt;/a&gt; before continuing on.&lt;/p&gt;

&lt;h3 id=&quot;loading-scalar-data-into-python&quot;&gt;Loading Scalar data into Python&lt;/h3&gt;

&lt;p&gt;Our first step is to import any libraries we’ll need. At the top of your new Python program, let’s add the following:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;json&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;bs4&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BeautifulSoup&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;networkx&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nx&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;math&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Here’s what the following libraries will help us with:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;json contains methods to import, parse, and export JSON in Python&lt;/li&gt;
  &lt;li&gt;BeautifulSoup is a library for parsing HTML inn Python. This will help us extract descriptions and links from our Scalar pages.&lt;/li&gt;
  &lt;li&gt;NetworkX contains a number of network analysis tools. We will use it to calculate betweenness centrality scores for our nodes (more later)&lt;/li&gt;
  &lt;li&gt;Math adds extra math functionality to Python. We will use it to round floats down to the nearest integer.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Next, we need a function to read the raw JSON dump from the Scalar API Explorer into a useful Python data structure. Below is a function that accepts JSON as input and returns the data as a dictionary for Python to use&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;loadScalarData&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# loads JSON data exported from the Scalar API Explorer tool&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# and parses as a dictionary for further manipulation with Python here&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;data_for_output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;r&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;encoding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;utf-8&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;read_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;json&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;strict&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;out_dict&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
            
            &lt;span class=&quot;c&quot;&gt;# also add the name of the current dictionary to the dcitionary itself and assign to the key 'key'&lt;/span&gt;
            &lt;span class=&quot;c&quot;&gt;# (because we're outputting a list of dictionaries, the key name would be lost otherwise)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;out_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'key'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;data_for_output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_dict&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data_for_output&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h3 id=&quot;structuring-scalar-data-as-a-network-of-nodes-and-links&quot;&gt;Structuring Scalar data as a network of nodes and links&lt;/h3&gt;

&lt;p&gt;Here’s where things get trickier. Our goal is to take this unfiltered, raw export of Scalar data and isolate the data that will be relevant for our network visualization: nodes (in the Scalar book, pages) and links (in the Scalar book, tags). To accomplish this task, we’ll use a few functions below.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;For this first function, please notes:  &lt;strong&gt;the function only looks for pages that are tagged at least once somewhere in the book&lt;/strong&gt;. I omit any page that is completely disconnected from all others. These disconnected, floating pages are sometimes called “orphan nodes”. To include orphan nodes would require a different method of generating the lists of links and nodes that I apply here, and is also outside the scope of our team’s visualization. That being said, you may find it useful to include these floating nodes in your own visualization. If so, please look for a function that includes orphan nodes in Part 2 of this tutorial.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;First, here is the function to generate a list of all tags and all (tagged) nodes:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;getTagsAndTaggedNodes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reference_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;excludeMedia&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# extracts all tags and pages that are tagged somewhere in the Scalar book&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# represents each item (tags/pages) as a dictionary&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# and returns a list for each item type&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;tags_for_network_links&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;tagged_pages_for_network_nodes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;


    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reference_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;current_item&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reference_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'urn:scalar:tag:'&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;current_item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'key'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;body&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;current_item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'http://www.openannotation.org/ns/hasBody'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'value'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;current_item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'http://www.openannotation.org/ns/hasTarget'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'value'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

            &lt;span class=&quot;c&quot;&gt;# ignore tags here that involve media nodes (almost always images)&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;excludeMedia&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'media'&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;body&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;or&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'media'&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                    &lt;span class=&quot;k&quot;&gt;continue&lt;/span&gt;
            
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;body&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tagged_pages_for_network_nodes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;tagged_pages_for_network_nodes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;body&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tagged_pages_for_network_nodes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;tagged_pages_for_network_nodes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            
            &lt;span class=&quot;n&quot;&gt;tag_dict&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                &lt;span class=&quot;s&quot;&gt;'source'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;body&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                &lt;span class=&quot;s&quot;&gt;'target'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
            
            &lt;span class=&quot;n&quot;&gt;tags_for_network_links&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tag_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tags_for_network_links&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tagged_pages_for_network_nodes&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;From poking around the Scalar output JSON, I figured out that the prefix indicating a tag is ‘urn:scalar:tag’. I further discovered that the “body” (or starting point) and “target” (or end point) of a tag is contained in current_item[‘http://www.openannotation.org/ns/hasBody’][0][‘value’] and current_item[‘http://www.openannotation.org/ns/hasTarget’][0][‘value’] . The goal of this program is to extract this data into tidy tags (or pairs of ‘body’ and ‘target’ values). Then as we go, we fill up the &lt;strong&gt;tagged_pages_for_network_nodes&lt;/strong&gt; variable with a list of every unique page we discover in our tags. At this point, we are keeping track only of the name of our pages, not any of their other attributes or content.&lt;/p&gt;

&lt;p&gt;Note that this function includes an &lt;strong&gt;excludeMedia&lt;/strong&gt; parameter that defaults to true. This is because, for our team’s visualization purposes, we did not want to include Media pages in the same way that we include non-media pages. However, if you would like to include both types of pages in your network visualization, simply add “excludeMedia=True” when you call &lt;strong&gt;getTagsAndTaggedNodes()&lt;/strong&gt; in the &lt;strong&gt;main()&lt;/strong&gt; loop (see Step 4 below).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;getTagsAndTaggedNodes()&lt;/strong&gt;returns two lists: &lt;strong&gt;tags_for_network_links&lt;/strong&gt; and &lt;strong&gt;tagged_pages_for_network_nodes&lt;/strong&gt;. While the tags are 100% ready to go at this point, we need to do further parsing of the nodes to gather all of the relevant .&lt;/p&gt;

&lt;h3 id=&quot;further-parsing-the-network-nodes-pages&quot;&gt;Further parsing the network nodes (pages)&lt;/h3&gt;

&lt;p&gt;Our list of nodes right now is just a collection of ids. We need to actually get all of the relevant data that we’ll need for our visualization. At this point, it’s helpful to ask the question: &lt;strong&gt;what do we actually want our users to see and/or be able to interact with?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In our case, our team would like our users to:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Read the name of people, cafes, and cities (stored in the name field of the relevant pages)&lt;/li&gt;
  &lt;li&gt;See a link the user can follow to the full Scalar page (stored as the URL of the Scalar page)&lt;/li&gt;
  &lt;li&gt;For pages with extra content like text and embedded photos, view a preview of this content on the sidebar (stored within embedded media information in the Scalar page. Note that this may require additional fiddling to work for your data).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here is the function to gather all the data relevant for those tasks (which is the most complex of all functions we’ll use in Part 1):&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;parseNodes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scalar_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pages_identified_in_links&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# fetch all relevant data about the pages and return as list of nodes&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# including checking for additional page data (text and embedded images)&lt;/span&gt;
    
    &lt;span class=&quot;c&quot;&gt;# note - you may wish to modify these parameters to include significant data&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# scraping tags is more straightforward, whereas metadata categories/etc. will be specific to your Scalar book&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;list_of_parsed_nodes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;node&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pages_identified_in_links&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;current_node_dict&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scalar_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;node&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;url&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;node&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;_id&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;node&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;_name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;imageURL&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;description&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;extraData&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;no&quot;&lt;/span&gt;

        &lt;span class=&quot;c&quot;&gt;# look for reference to media&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;references&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;current_node_dict&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;http://purl.org/dc/terms/references&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;imageURL&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;description&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;references&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;media&quot;&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;references&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;value&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
                &lt;span class=&quot;c&quot;&gt;# if reference to media found, fetch URL and description from media page&lt;/span&gt;
                &lt;span class=&quot;c&quot;&gt;# using BeautifulSoup to parse the HTML descriptions&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;content&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scalar_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;node&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;http://rdfs.org/sioc/ns#content&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'value'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;soup&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BeautifulSoup&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;content&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;html.parser&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;imageURL&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;soup&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;find&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;a&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;href&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;description&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;soup&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;

                &lt;span class=&quot;c&quot;&gt;# ceate an abbreviated description (to encourage users to go to Scalar for full description)&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;description&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;80&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;description&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;description&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;...&quot;&lt;/span&gt;
        
        &lt;span class=&quot;c&quot;&gt;# when directing the user to the Scalar page via the &quot;url&quot; attribute in the visualization&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;# we can ignore the extra version suffix, such as &quot;cafe_central.4&quot; and simply send them to&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;# &quot;cafe_central&quot; -- otherwise Scalar may omit the page description (for some reason)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'.'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;url&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;elif&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'.'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;url&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;elif&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'.'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;url&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        
        &lt;span class=&quot;k&quot;&gt;try&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;c&quot;&gt;# make sure the node is fetching the name correctly, assign to _name&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;_name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;current_node_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'http://purl.org/dc/terms/title'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'value'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;except&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;_name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;node&lt;/span&gt;

        &lt;span class=&quot;c&quot;&gt;# assemble a dictionary to describe the current node&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;node_out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;s&quot;&gt;'id'&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;s&quot;&gt;'name'&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;s&quot;&gt;'url'&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;s&quot;&gt;'colour'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;	&lt;span class=&quot;s&quot;&gt;&quot;#2a2a2a&quot;&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

        &lt;span class=&quot;c&quot;&gt;# optional imageURL and description paramaters for the nodes, if the pages include this data&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;# will also include an &quot;extraData&quot; parameter to indicate if either attribute exists, for use by&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;# the network viz later on&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;imageURL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;node_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;imageURL&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;imageURL&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;extraData&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;yes&quot;&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;description&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;node_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;description&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;description&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;extraData&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;yes&quot;&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;node_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;extraData&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;extraData&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;list_of_parsed_nodes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;node_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;list_of_parsed_nodes&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;This function works by iterating through the list of tagged pages. For every page, we start to construct all of the variables we’ll need to describe our nodes, such as the name, the URL, etc. Each of these variables eventually gets stored in a single dictionary, &lt;strong&gt;node_out&lt;/strong&gt;, and appended to our working variable &lt;strong&gt;list_of_parsed_nodes&lt;/strong&gt;. After iterating through every page, we can then return the &lt;strong&gt;list_of_parsed_nodes&lt;/strong&gt; variable with all of the data we need to describe the nodes in our network.&lt;/p&gt;

&lt;p&gt;You may notice a few peculiar blocks of code. First, I included a block of code to deal with text descriptions and embedded images. If your page includes this kind of data, you may wish to include it along with other information about your nodes. That way, later on when you visualize the network with JavaScript, you can use the text and images to generate a nice little preview (in our case, within the sidebar). However, the text and/or image appear in the Scalar API Explorer output JSON as a string of raw HTML. This is probably overkill for our purposes - it would be better to isolate the text and isolate an embedded URL.&lt;/p&gt;

&lt;p&gt;BeautifulSoup is a Python library that excels in just this situation: parsing website HTML in such a way that Python can quickly identify the meaningful parts and pass them along to simpler variables. We do that here in the if “media” block. &lt;strong&gt;Note: this is another area you may wish to rewrite a bit, depending on how your Scalar book is structured.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Second, I wrote a series of if statements to deal with stripping out the page version suffix of pages. In Scalar, if you continue to edit pages, the platform will automatically create new versions of the page such as “page.2”, “page.3”, and so on. The most recent version is included in the tags and in the names here. However! I’ve noticed that dropping a URL into your browser with that “page.3” ending leads the user to a strange limited version of a Scalar page. To get around this issue, we can remove the “.3” suffix entirely, and the URL of “scalar.usc.edu/blahblahblah….page” will go to the correct place.&lt;/p&gt;

&lt;p&gt;At the end of this function call, it will return a list of nodes - but unlike the first list that contained only names, we now have a list of dictionaries packed with (almost) all of the information about the nodes we’ll need for our network visualization.&lt;/p&gt;

&lt;h2 id=&quot;step-3-add-extra-network-analysis-indicators-to-the-visualization&quot;&gt;Step 3: Add extra network analysis indicators to the visualization&lt;/h2&gt;

&lt;p&gt;At this point, we could call it a day and declare our node and link lists complete. However! I found it helpful to add one more piece of data to our network: betweenness centrality scores.&lt;/p&gt;

&lt;p&gt;Why betweenness centrality? Consider the question: how important is any one node to the network as a whole? You might say “the node in the middle is the most important.” But where exactly is the “middle”? And is the concept of “in the middle” a sufficient indicator of what makes any node important to the network as a whole?&lt;/p&gt;

&lt;p&gt;Imagine for a moment a city with a subway or train network. You may imagine several subway lines running from the far edges of the transit network (where the stops are more isolated) into the downtown area. Closer to downtown, more and more lines may criss-cross at any given subway stop. Which of these subway stops is most important? Let’s imagine one of these downtown stops is suddenly closed due to a mechanical failure. One way you might describe the degree of inconvenience of this closure is to say “how many train routes will now be impossible for a commuter to travel? Or how many will require a lengthy detour?”&lt;/p&gt;

&lt;p&gt;Betweenness centrality is a measure of exactly that concept: of all possible routes an individual could travel from starting-point to end-point in the network, how manny of the most efficient routes (the kind we usually take to commute somewhere) travel through a given node? If the number is very high, we can say that this node has a high betweenness centrality score. If it’s low, we say it has a low score. (If I’ve piqued your interest, here’s an &lt;a href=&quot;https://cambridge-intelligence.com/keylines-faqs-social-network-analysis/&quot;&gt;interesting writeup with visualizations&lt;/a&gt; of BC and other social network measures.)&lt;/p&gt;

&lt;p&gt;How do we add betweenness centrality to our network? We could wait until the JavaScript visualization step to generate these scores – but then we have to waste the user’s time calculating the numbers. Instead, let’s include a process now to add those scores to the nodes ahead of time.&lt;/p&gt;

&lt;p&gt;First, we need to generate the scores. Luckily, the NetworkX network analysis package in Python has a pithy way to accomplish this goal. Below is a function using NetworkX (which we imported to our program as &lt;strong&gt;nx&lt;/strong&gt;) to generate scores:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;generateBetweennessCentrality&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;final_list_links&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;final_list_nodes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# use networkx to generate betweennenss centrality for nodes and return as dict&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;nodes_for_nx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;node&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'id'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;node&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;final_list_nodes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;edges_for_nx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'source'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'target'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;final_list_links&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;G&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Graph&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;G&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_nodes_from&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nodes_for_nx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;G&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_edges_from&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;edges_for_nx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;betweenness_centrality&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;G&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;At this part of the process, you may wish to add even more network analysis measures. If you’re curious, NetworkX also includes a rudimentary &lt;a href=&quot;https://networkx.github.io/documentation/networkx-1.10/reference/drawing.html&quot;&gt;network viz drawing tool&lt;/a&gt;, which would help you generate a visualization directly from Python! However, D3 is much more capable at generating pretty and interactive visualizations, as you will see (I hope!) in Part 2.&lt;/p&gt;

&lt;p&gt;We also need to write one more piece of functionality: adding the betweenness centrality scores of our nodes to our list of node dictionaries. This is slightly inconvenient given the way we’ve structured our node data as a list of dictionanries (in a more complex program, it might be a good time to ask the question: should I have created classes at some point?) Here, we can iterate over each node in our list, add the attribute, and reassemble and return our new list:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;addBetweennessCentralityToNodes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;betweenness_centrality_scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;final_list_links&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;final_list_nodes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# add BC scores as a new parameter, 'betweeness_centrality_score'&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# as an integar between 0 and 100 to use in network viz (as font size, etc.)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;final_output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;final_output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'links'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;final_list_links&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;final_output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'nodes'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;node&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;final_list_nodes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;current_bc_score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;betweenness_centrality_scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;node&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'id'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;# let's convnert this percentage value to an integer between 0 to 100, roundng up &lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;node&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'betweenness_centrality_score'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ceil&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;current_bc_score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;final_output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'nodes'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;node&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;final_output&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2 id=&quot;step-4-gather-and-export-data-as-single-json-file&quot;&gt;Step 4: Gather and export data as single .json file&lt;/h2&gt;

&lt;p&gt;It’s time to bring it all together! We have all of the pieces we need. Now we need to write a single &lt;strong&gt;main())&lt;/strong&gt; function that calls the functions, stores their output as we go, and exports the single final output as a .json file.&lt;/p&gt;

&lt;p&gt;Note that in Python, the &lt;strong&gt;main()&lt;/strong&gt; function is a way to encapsulate everything we wish to do in a program when we call it from the command line. When paired with an &lt;strong&gt;if __name__ == “__main__” : main()&lt;/strong&gt; block, the &lt;strong&gt;main()&lt;/strong&gt; function will be called when you run the Python program from the command line. This also helps prevent a common problem if you were to import &lt;strong&gt;generate_scalar_network.py&lt;/strong&gt; as a module in another program (in the same way we imported json, NetworkX, etc.) Python will actually go ahead and immediately run anything in a module that isn’t contained within a function. As a result, if we have our code dangling outside of the &lt;strong&gt;main()&lt;/strong&gt; function, it will behave just as planned when we run it from the command line… but may do strange, out-of-order things if we import it in our program. (We can still call main() in the future even after importing the program as a module, however! We would do this by calling generate_scalar_network.main())&lt;/p&gt;

&lt;p&gt;Here is the code we’ll need to carry out our program, all packaged in a tidy &lt;strong&gt;main()&lt;/strong&gt; function, as well as the &lt;strong&gt;name&lt;/strong&gt; block discussed above:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;# set filenames for input .json (from Scalar API explorer) and output .json&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;input_file&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;scalar_output.json&quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;output_file&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;clean_scalar_data.json&quot;&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;# load Scalar data .json file into Python data&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;scalar_data_dict&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loadScalarData&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;# generate final list of links and nodes&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;final_links&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;linked_pages&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;getTagsAndTaggedNodes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scalar_data_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;final_nodes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;parseNodes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;scalar_data_dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;linked_pages&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;# generate betweenness centrality scores for nodes using NetworkX &lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;betweenness_centrality_scores&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;generateBetweennessCentrality&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;final_links&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;final_nodes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;# one final step to collect links and nodes (with betweenness-centrality scores added) into a single dictionary&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# (this is assuming this is the most useful way to pass the data to a network visaulization&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# BUT you may wish to modify or rewrite this function if your visualization calls for another way to organize)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;final_output&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;addBetweennessCentralityToNodes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;betweenness_centrality_scores&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;final_links&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;final_nodes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;w&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;write_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;json&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dump&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;final_output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;write_file&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'clean data successfully written to disk as &quot;clean_scalar_data.json&quot;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__name__&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;__main__&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;And you’re done!! If everything went as planned, when you run your &lt;strong&gt;generate_scalar_network.py&lt;/strong&gt; script in a directory (and remember to plop in the &lt;strong&gt;scalar_output.json&lt;/strong&gt; dump from the Scalar API Explorer beforehand) you should see a tidy &lt;strong&gt;clean_scvalar_data.json&lt;/strong&gt; file in the same folder.&lt;/p&gt;

&lt;p&gt;To view your JSON, you have a few options. You can always open it up in your text editor of choice. Mozilla Firefox actually does an excellent job of visualizing JSON and letting you explore the hierarchy a bit. Here’s a sample of what that looks like:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/img/scalar-network/JSON_in_firefox.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The Online JSON Viewer provides a very similar feature – copy and paste the JSON text data into the field in the Text tab, and then switch to the Viewer tab in the top-left to visualize and explore the results.&lt;/p&gt;

&lt;p&gt;If you’d rather download a ready-to-go version of the code excerpts above, you may also download &lt;a href=&quot;https://github.com/zoews/interactive-network-viz/blob/master/parsing-tools/generate_scalar_network.py&quot;&gt;generate_scalar_network.py&lt;/a&gt; using the Raw link at this GitHub page. You can also test it out on an version of our Scalar book’s export data at &lt;a href=&quot;https://github.com/zoews/interactive-network-viz/blob/master/parsing-tools/scalar_output.json&quot;&gt;scalar_output.json&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;If something goes wrong, don’t give up! It’s likely means that there’s something about your scalar book export that requires a little more fiddling in the way we’ve implemented the functions here. Feel free to shoot me an e-mail if you feel totally stuck too!&lt;/p&gt;

&lt;h2 id=&quot;wrapping-up&quot;&gt;Wrapping Up&lt;/h2&gt;

&lt;p&gt;You’ve now got some network data to work with! If you’re comfortable getting a JavaScript excerpt up and running, you could even plug in this network JSON into the &lt;a href=&quot;https://bl.ocks.org/mbostock/ad70335eeef6d167bc36fd3c04378048&quot;&gt;bl.ocks network visualization example&lt;/a&gt; we mentioned above. In Part 2, we will explain in detail how to integrate these pieces and pretty radically redesign this code excerpt to work better in highlighting your Scalar data.&lt;/p&gt;

&lt;p&gt;Congratulations for making it through! More next time. And in the meantime: happy experimenting!&lt;/p&gt;
</description>
        <pubDate>Tue, 21 Aug 2018 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/articles/2018-08/custom-scalar-pt1</link>
        <guid isPermaLink="true">http://localhost:4000/articles/2018-08/custom-scalar-pt1</guid>
        
        
        <category>tutorial</category>
        
      </item>
    
      <item>
        <title>What does critical data science add to our understanding of sexual harassment in academia?</title>
        <description>&lt;p&gt;&lt;em&gt;A cautious introduction to NLP and Machine Learning methods in analyzing thousands of anonymous sexual harassment &amp;amp; assault reports.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;“The data are too messy.”&lt;/p&gt;

&lt;p&gt;“There’s no way we could work through it in time.”&lt;/p&gt;

&lt;p&gt;“I’d like to figure something out. But I don’t know.”&lt;/p&gt;

&lt;p&gt;I was sitting in the grad student lounge in Ann Arbor with three classmates from Information Visualization. Each of us had the same Google Sheet pulled up on our browsers: “&lt;a href=&quot;https://docs.google.com/spreadsheets/d/1S9KShDLvU7C-KkgEevYTHXr3F6InTenrBsS9yk-8C5M/edit#gid=1530077352&quot;&gt;Sexual Harassment In the Academy: A Crowdsource Survey. By Dr. Karen Kelsky, of The Professor Is In&lt;/a&gt;”.&lt;/p&gt;

&lt;p&gt;In just a few months, a &lt;a href=&quot;http://theprofessorisin.com/2017/12/08/our-metoophd-moment/&quot;&gt;call for anonymous survey submissions&lt;/a&gt; in the popular The Professor Is In blog had resulted in over 2,300 reports of sexual harassment and assault.&lt;/p&gt;

&lt;p&gt;Our group quickly realized a few things: this data was immense. It was messy in the sense that data folks often describe messy data: non-standard, full of missing values and strange capitalizations. It was also data that spoke to an immensity of pain, the loss of futures diverted and destroyed.&lt;/p&gt;

&lt;p&gt;One peer described the issues in the data, column by column, but she stopped when it came to the “Event” column. We reached a point where we stopped knowing what to say, and just made eye contact with each other. Eventually, our group passed on the Sexual Harassment dataset in favor of a climate change-related project (which generated its own share of complex data issues).&lt;/p&gt;

&lt;p&gt;However, I couldn’t stop thinking about the survey. Thousands of individuals had revealed their experiences with sexual harassment and assault in academia, many apparently for the first time. These reports detailed the devastating effect these events had on their lives. This is vital, powerful data that deserves to have its story told.&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;When I had the opportunity to propose an individual project for my Data Manipulation &amp;amp; Analysis course, I petitioned successfully to build something around the sexual harassment data. This post describes my semester-long attempt to use the toolkit of natural language processing and machine learning - specifically, the tools of &lt;strong&gt;topic modeling&lt;/strong&gt; and &lt;strong&gt;text classification&lt;/strong&gt; - hand-in-hand with a reflexive process of generating, challenging, and revising research questions. It also describes my ongoing process of finding someething deeper: a critical data science approach that challenges systems of power, and highlights rather than flattens individual experience.&lt;/p&gt;

&lt;h3 id=&quot;the-tldr&quot;&gt;The tl;dr:&lt;/h3&gt;

&lt;p&gt;I’m going to take a bit to describe this journey below. If you don’t have the time to wade through with me, here are the &lt;strong&gt;key findings of my analysis&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Topic modeling draws our attention to recurring elements of the reports: the supervisor/mentor relationships, common spaces like the research lab, ongoing “pressuring” and “grooming” behaviors by perpetrators, and the interrelationship of physical/mental/emotional/career impacts on individual lives.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Specifically, topic modeling encourages us to ask: what is the domain of places, relationships, and interactions at play in these incidents? What may we have failed to include or anticipate in our data collectionn strategies?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If we collect information about institutional responses, or description of the incident itself, we can predict with ~80% accuracy whether the target will eventually “quit” or “leave” their career/academia/etc. in the long run. Non-responses from institutions are among the most predictive of a left/quit outcome (although this method does not, in itself, establish the direction or causality of such a relationship).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Building a predictive model doesn’t have to be a means to an end. Rather, it gives us an opportunity to assess which features of our data are most powerful when making those predictions, and in doing so, helps us to identify potential correlations to focus on in subsequent investigations.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here’s some of what I hope you &lt;strong&gt;take away&lt;/strong&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Critical data science, as a set of methods and an ethical stance, can absolutely play a part in challenging our assumptions, redirecting our attention, and prompting us to ask new and more careful questions.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Our intuitions and findings in this realm are always contingent. We must always contextualize what we produce within the day-to-day and institutional realities in which some narratives are given more power than others.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;I want to use data science to interrogate and challenge structures of power. (And I hope you want that, too!)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now, how did we get here?&lt;/p&gt;

&lt;h2 id=&quot;the-initial-question&quot;&gt;The Initial Question&lt;/h2&gt;

&lt;p&gt;I knew that no matter what results this analysis produced, it would have to grapple with a basic question: &lt;strong&gt;what does NLP/machine learning contribute that makes it worth paying attention to when we have the words of targets &amp;amp; survivors in front of us?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;This question had two main implications for me. First, it pointed to the possibility that there may be aspects of the experiences of these 3,000+ individuals that we may not be able to grasp through rote, line-by-line reading alone. I mean &lt;em&gt;possibility&lt;/em&gt; here sincerely, as I was unsure what those elusive structures might hold, if anything.&lt;/p&gt;

&lt;p&gt;Second, it called attention to the very real impact of prioritizing some types of data over others. As my mentor Justin Joque observed when I discussed this project with him this spring, administrators and other gatekeepers use the idea of threshold of objectivity to dismiss individual testimony of sexual harassment and assault. (The wider public uses similar rhetoric when using legal thresholds to reject individual reports as slander, not containing sufficient evidence, etc.) By claiming that algorithms can unearth new data from text responses in the aggregate, we must be careful not to reinforce this toxic idea that narratives alone are never enough to identify and combat systematic abuse.&lt;/p&gt;

&lt;p&gt;Data science as a way of producing insight also brings with it an ideology prevalent in statistics and computational methods: &lt;em&gt;this thing is real because we’ve measured it&lt;/em&gt;. This is especially powerful in something like textual survey responses. No matter how real these words felt to my group members and myself sitting around a table in Ann Arbor, most university discipline boards and other institutional actors ascribe greater power to what they perceive as objective evidence. At the same time, identifying implicit structures and other &lt;em&gt;unknown unknowns&lt;/em&gt; can be an impetus to turn our attention back on these narratives with renewed intensity. Therein lies the opportunity, and the risk, in exploring texts of individual narratives.&lt;/p&gt;

&lt;h2 id=&quot;the-dataset&quot;&gt;The Dataset&lt;/h2&gt;

&lt;p&gt;The Sexual Harassment in Academia dataset is a collection of responses to an open Google Survey. For my analysis, I download the data on April 4, 2018, when 2,438 reports had been submitted between December 1st, 2017 and April 4, 2018:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://people.csail.mit.edu/karger/Exhibit/Harass/&quot;&gt;Here is a website&lt;/a&gt; providing a faceted interface for accessing the data (the introduction also linking to the survey results, a blog post by its creator, etc.)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/img/harassment/image1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This version uses a slightly modified version of the original survey results adds a few additional columns that attempt to clean or otherwise aggregate data. However, at the time of analysis those standardization attempts were still scattershot and only partially implemented (and as of July 2018 continue to be in-progress).&lt;/p&gt;

&lt;p&gt;The survey includes only two categorical variables, presented as multiple-choice options: gender of perpetrator (woman/man/non-binary/unsure/various/other__) and type of institution (small liberal arts college/Elite Institution/Ivy League/Other R1/etc.) Otherwise, responses are all open text fields, and many are optional (or accept a blank or “none” as submission).&lt;/p&gt;

&lt;p&gt;These text field submissions include:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Event description, target status (e.g. student, professor), perpetrator status (e.g. professor/chair/fellow graduate student/etc.), name of the institution (optional), field discipline, institutional response (if any), institutional/career consequences for perp (if any), impact on your career, impact on your mental health, impact on life trajectory/choices, and other.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In my analysis, I make a distinction between data that describes the nature of the encounter (such as role of perpetrator and event description), the consequences for the perpetrator (response, punishment), and the long-term impact on the target (mental health, career, and life impacts). The survey does an excellent job in affording us a vivid picture of harassment across time, and laying the groundwork for investigating the correlations between aspects of the event and what comes after.&lt;/p&gt;

&lt;h2 id=&quot;the-research-questions&quot;&gt;The Research Questions&lt;/h2&gt;

&lt;p&gt;With these concerns in mind, I attempted to surface elements in the survey responses that may not have been visible from the texts alone - but that also draw the researcher’s attention &lt;em&gt;back into the reports&lt;/em&gt; rather than abstracting the specificity away. I include four research questions in the original report (see the Notes below), but here I’m going to focus on three research questions that I’ve gathered into two categories:&lt;/p&gt;

&lt;h3 id=&quot;topic-modeling-of-responses&quot;&gt;Topic modeling of responses:&lt;/h3&gt;

&lt;p&gt;Q1: What are the main themes we might infer from reports of harassment/assault events? (&lt;em&gt;Note: will include a discussion of topic vs theme below&lt;/em&gt;)&lt;/p&gt;

&lt;h3 id=&quot;feature-analysis-of-classifiers&quot;&gt;Feature analysis of classifiers:&lt;/h3&gt;

&lt;p&gt;Q2: What language in incident descriptions is most predictive of whether the perpetrator was a professor/faculty/department chair or not? How powerful is this prediction?&lt;/p&gt;

&lt;p&gt;Q3: What language in the incident report is most predictive of whether the respondent “left” or “quit” their position/academia/the discipline/etc? How powerful is this prediction (in general and relative to the professor model)?&lt;/p&gt;

&lt;p&gt;Below, I will describe the rationale for each approach and the methods I used to take them on.&lt;/p&gt;

&lt;h2 id=&quot;methods-topic-modeling&quot;&gt;Methods: Topic Modeling&lt;/h2&gt;

&lt;p&gt;“I’m wondering not only &lt;em&gt;what&lt;/em&gt; the topics are, but &lt;em&gt;of what order&lt;/em&gt;…”&lt;/p&gt;

&lt;p&gt;A doctoral student I had worked with on topic modeling posed this question in his presentation last week. It struck me as an exceptionally sharp way to articulate something I had only just started exploring in my own project. That is, what do the topics in topic modeling reveal about the domain of responses in these surveys, their types, and their relative position to one another with regard to hierarchies of all kinds? I was curious about experiences, attitudes, beliefs, and other elements I had not anticipated ahead of time.&lt;/p&gt;

&lt;p&gt;In order to generate topics, I needed to prepare the surveys to go through a machine learning pipeline in Python, which I decided to implement in Apache Spark. This consisted of several steps, which I’ll describe in broader terms and with a few notes on implementation:&lt;/p&gt;

&lt;h3 id=&quot;cleaning-and-preprocessing-the-data&quot;&gt;Cleaning and preprocessing the data.&lt;/h3&gt;

&lt;p&gt;I stored values within a Spark DataFrame structure (which behaves similarly to tables in SQL). After simplifying and standardizing the text (lower case, no punctuation), and battling nulls and missing values, it was ready for the pipeline.&lt;/p&gt;

&lt;h3 id=&quot;building-the-pipeline&quot;&gt;Building the pipeline&lt;/h3&gt;

&lt;p&gt;I decided upon Latent Dirichlet allocation(LDA) as my algorithm of choice for generating models. Here is a &lt;a href=&quot;https://opendatascience.com/topic-modeling-with-lda-introduction/&quot;&gt;helpful primer&lt;/a&gt; on the intuition behind generating topics with LDA.&lt;/p&gt;

&lt;p&gt;To get LDA up and running, I needed to construct a full machine learning pipeline. I used the MLLib library in PySpark (the Python implementation of the Apache Spark cluster-computing framework). While there are tradeoffs to using MLLib as opposed to a single-process approach (e.g. with TensorFlow), I wanted to teach myself more about distributed computing and the Spark way of doing things in this project. I also wanted to implement pipelines in a way that could scale to much bigger datasets later on. (I used the Databricks cloud-hosted platform to run my Spark code, but look for a future blogpost about running Spark/MLLib locally in JupyterLab using the BeakerX extension.)&lt;/p&gt;

&lt;p&gt;The PySpark MLLib pipeline consisted of (1) tokenizing all words within a particular column, such as “event” description, (2) removing stopwords, (3) implementing a vectorizer (CountVectorizer) in order to represent each entry as a count of unique words, and (4) implementing a LDA model that outputs k topics for column c in the data.&lt;/p&gt;

&lt;p&gt;In this pipeline, a paragraph of text response is converted to a wordcount of each unique words (steps 1-3). Stopwords are discarded at step 2, and any words that fail to meet a minimum threshold of frequency across all responses (i.e. any words that fall below the top 3,000 most frequent words in the corpus) are also discarded by the CountVectorizer function (step 3). The output is a list of topics, with words listed in order of their relevance to the topic (the first or left-most word being the most significantly correlated with the topic, which is not given a specific name).&lt;/p&gt;

&lt;h3 id=&quot;tweaking-parameters&quot;&gt;Tweaking parameters&lt;/h3&gt;

&lt;p&gt;I spent some time adjusting the number of topics. While I wanted to maintain low log perplexity and high log likelihood (my two evaluation metrics), I also wanted to maximize (subjective) intelligibility for a human reader. I settled on k=15 as a balance between acceptable evaluation metrics and high coherence for a human reader.&lt;/p&gt;

&lt;h3 id=&quot;analysis&quot;&gt;Analysis&lt;/h3&gt;

&lt;p&gt;Finally, with the topic modeling pipeline constructed, I fed in survey responses from each response category individually, in order to highlight patterns more closely.&lt;/p&gt;

&lt;h2 id=&quot;methods-feature-analysis-of-classification&quot;&gt;Methods: Feature analysis of classification&lt;/h2&gt;

&lt;p&gt;While the survey results included very few categorical variables relative to the many open text-field responses, there were nonetheless clear patterns in those text responses that suggested possible categorizations.&lt;/p&gt;

&lt;p&gt;Two jumped out at me early on. First, while there wasn’t a controlled vocabulary for the “perpetrator” response, there was nonetheless a very clear subset of perpetrators who were faculty, staff, department chairs or heads, etc., and non-faculty perpetrators. The survey had thus anticipated the role of the perpetrator would be important, but hadn’t included a clear categorical option for describing that perpetrator’s faculty status.&lt;/p&gt;

&lt;p&gt;Second, I noticed a disturbing trend: many of the survey respondents described quitting or leaving their job or career in the life consequences response fields. That so many individuals described their life so profoundly altered by these acts of harassment and assault struck me as of profound importance. While the survey had attempted to capture life impacts in general terms, it again did not anticipate this profound outcome through use of a categorical response.&lt;/p&gt;

&lt;p&gt;I thought a lot about these two implicit categories. How could I explore their relationship to other aspects of the survey response? I took inspiration from the idea of &lt;em&gt;feature importance analysis&lt;/em&gt; which appeared in an earlier assignment. The idea: when building a model to predict the correct class of some set of data, the ultimate goal isn’t necessarily to have that model prepared and ready for data in the future. Instead, you can use the modeling process as a way to measure &lt;strong&gt;what features of the data are most significant in predicting an outcome relative to all other features&lt;/strong&gt;. You can also determine the overall strength of the model.&lt;/p&gt;

&lt;p&gt;The intuition from there is that: if you build a model for predicting one categorical variable in your data based on some other data, and the model has a high accuracy, you may wish to investigate the most important features used in this model. Why were these features so successful in producing the right classification? What’s going on here?&lt;/p&gt;

&lt;p&gt;My main interest was to see to what degree I could generate a realistically predictive model for each of the two categories (prof/non prof, or left/not left) based on the incident descriptions. If so, what specific fields of description (such as the event, the punishment, etc.) most strongly predicted professor/no-professor perpetrators? And given those fields, what features (in this case, words within the description) are the strongest predictors within the model?&lt;/p&gt;

&lt;h3 id=&quot;generate-categorical-variablesextract-features&quot;&gt;Generate categorical variables/extract features&lt;/h3&gt;

&lt;p&gt;To categorize entries as professor and non-professor, I added a new column, “IsProf”, which contained a boolean value indicating if the substrings “prof”, “faculty”, or “chair” were found in the perpetrator field. I then followed the same procedure to generate isLeft based on “left” and “quit” appearing in the life and career impact columns.&lt;/p&gt;

&lt;p&gt;(After some trial and error eventually realized that MLLib performed better if IsProf and IsLeft were represented as 1/0 instead of True/False. Spark was tricky in this way, among others!)&lt;/p&gt;

&lt;h3 id=&quot;construct-pipeline-to-support-classification&quot;&gt;Construct pipeline to support classification&lt;/h3&gt;

&lt;p&gt;I approached this question as a machine learning/classification task, and built a classifier pipeline using PySpark with a Random Forest classifier. Here is a helpful introduction to &lt;a href=&quot;https://towardsdatascience.com/the-random-forest-algorithm-d457d499ffcd&quot;&gt;Random Forest classification&lt;/a&gt; that also touches on the concept of feature importance.&lt;/p&gt;

&lt;p&gt;To carry out classification, I establishes a new pipeline using PySpark’s MLLib library. I used many elements of the earlier LDA topic modeling pipeline, such as starting with stop words removal and tokenization. As with LDA, I included a CountVectorizer vectorizer in order to represent entries in a given column as a collection of word counts, given a specific size of dictionary and minimum document frequency for terms. These counts act as features that I then passed into a Random Forest classifier (which is, technically, a meta-classifier of tree classifiers). Finally, I set the one-hot “IsProf” variable as the target label for classification.&lt;/p&gt;

&lt;p&gt;This CountVectorizer + Random Forest approach felt like a clear way to pursue classification without further transforming the feature set (for example, I thought about but eventually decided against word embeddings, as I wasn’t sure if the semantic assumptions of a word embedding approach would overpower the data given the small-moderate sample size).&lt;/p&gt;

&lt;h3 id=&quot;training-and-testing&quot;&gt;Training and testing&lt;/h3&gt;

&lt;p&gt;I trained and tested the model on a 80/20 split of the input data (after first forgetting to create a training set! I describe this in the results section.)&lt;/p&gt;

&lt;h3 id=&quot;output-feature-importance-and-overall-model-strength&quot;&gt;Output feature importance and overall model strength&lt;/h3&gt;

&lt;p&gt;I outputted accuracy as a percentage of successful predictions, and finally outputted a list of features ranked by importance in the model. I printed the success rate to screen and visualized the features ranked by importance as a seaborn barplot.&lt;/p&gt;

&lt;h2 id=&quot;the-findings&quot;&gt;The Findings&lt;/h2&gt;

&lt;h2 id=&quot;q1-what-are-the-main-themes-or-topics-in-reports-of-harassmentassault-event&quot;&gt;Q1: What are the main themes (or topics) in reports of harassment/assault event?&lt;/h2&gt;

&lt;p&gt;I described above the importance of interpreting topics as possible relationships, hierarchies, and domains of inquiry. The topics that resulted from this process certainly suggest a number of points of investigation within each sub-corpus – in this case, the total set of responses to a given question. I believe these findings may correspond to real patterns in incidents, outcomes, and impacts worth investigating further.&lt;/p&gt;

&lt;p&gt;To illustrate patterns in the context of the harassment/assault event, here is a table of topics from the “event” field of responses:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/img/harassment/image2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;These topics provoke a number of questions. Is there an association between the “dean” position and long-term, sustained harassment (“years”, “several”, “involved”?) When events appear to a peer (“student”, “grad”, “department”) in the final topic, why does “lab” show up frequently? How might “removed” or “separate” relate to a common strategy of shuffling around perpetrators without terminating employment? The relationship between “pressured, “raped,” and “groomed” seems to tell a specific story – one in which a perpetrator takes deliberate steps over time before committing sexually violent acts.&lt;/p&gt;

&lt;p&gt;The topics from the “mental” field of responses illustrates the profound impact of these events on mental health and wellbeing:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/img/harassment/image3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The impacts described above seem truly devastating. They range from a mixture of emotional and physical responses (“stress”, “migraines”, “pain”) to the loss of trust in those in powers (“distrust”, “figures”, “authority”, “male”). Mental health disorders appear together (”anxiety”, “depression”, “ptsd”). Meanwhile, the topic led with “male” includes the mildest emotion across all topics: “annoyed”.&lt;/p&gt;

&lt;p&gt;These responses demonstrate how interrelated physical, cognitive, and emotional impacts of traumatic events can be. They suggest that perhaps a single diagnostic category like an anxiety disorder or depression may be insufficient when trying to anticipate possible mental health repercussions from targets of sexual harassment and assault.&lt;/p&gt;

&lt;p&gt;Finally, here is a table of topics in the punishment category:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/img/harassment/image4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In these responses, we find very little evidence of a repercussion like firing, although “removed” does appear once. Instead, we see a prominent pattern of topics that begin with “none”, “unknown”, “na”, “nothing”, or even just an empty string (a non-response). We even see evidence of positive academic outcomes for faculty such as “promoted”, and “get” “tenure”.&lt;/p&gt;

&lt;p&gt;The topics also point to “report” “didn’t”, which echoes some observations from n-gram analysis in the original study that suggest non-reporting by targets may be one aspect of the story. While non-reporting does not appear elsewhere in these results, it is an important aspect to consider – what are the factors that influence the willingness of targets to make a formal report?&lt;/p&gt;

&lt;p&gt;These topic results alone do not provide conclusive evidence of the distribution of descriptions for these categories. However, they do sketch out the domain of possibilities and show relationships that tell cohesive stories about common elements in these narratives.&lt;/p&gt;

&lt;h2 id=&quot;q2-what-language-in-incident-descriptions-is-most-predictive-of-whether-the-perpetrator-was-a-professorfacultydepartment-chair-or-not-how-powerful-is-this-prediction&quot;&gt;Q2: What language in incident descriptions is most predictive of whether the perpetrator was a professor/faculty/department chair or not? How powerful is this prediction?&lt;/h2&gt;

&lt;p&gt;After much tweaking, the first classification model used to predict professor/not professor perpetrators resulted in modest success. The most accurate prediction occurred with the “event” column – however, I chose to exclude this result given that the most accurate feature by far was “professor,” and simply mentioning a professor in both the perpetrator and event field didn’t feel meaningful in any particular way. I instead focused on the perpetrator outcome and target impact.&lt;/p&gt;

&lt;p&gt;At first, I thought I had achieved a very strong result: accuracies in the range of .72 to .80 (that is, 72% to 80% of the test subset being classified correctly)! However, I noticed my mistake: I had failed to create a distinct test and training sets. The model had clearly been overfitted.&lt;/p&gt;

&lt;p&gt;Once I created a test/train split correctly (oof), the correctly fit model ranged from .59 to .637 accuracy in predicting professor status.&lt;/p&gt;

&lt;p&gt;I felt that, given these very low numbers in terms of classification success, it would be misleading to try to interpret the featureImportance figures in great detail. For instance, here is the raw output of the most successful classification task (.637 accuracy) which occurred with the mental health impacts category:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/img/harassment/image5.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I am not sure how to interpret this outcome. Mentioning “think”, or a blank entry (somehow this persisted even after attempting to remove nulls at length!) do not easily lead to a real-world interpretation, and “academia” and “women”, while interesting, elude easy interpretation as well.&lt;/p&gt;

&lt;p&gt;The main finding from this question is that the survey descriptions only weakly predict professor/no professor. Perhaps another approach would lead to a more successful model, but at the moment, the relationship doesn’t appear to be very strong.&lt;/p&gt;

&lt;h2 id=&quot;q3-what-language-in-the-incident-report-is-most-predictive-of-whether-the-respondent-left-or-quit-their-positionacademiathe-disciplineetc-how-powerful-is-this-prediction-in-general-and-relative-to-the-professor-model&quot;&gt;Q3: What language in the incident report is most predictive of whether the respondent “left” or “quit” their position/academia/the discipline/etc.? How powerful is this prediction (in general and relative to the professor model)?&lt;/h2&gt;

&lt;p&gt;In the previous step, we achieved classification success of just .59 to .637 in the attempt to predict professor status of perpetrator. In contrast, I discovered that the task of predicting the target leaving or quitting a job/academia/etc. based on either description was a much higher success rate of 79.3%! This signifies much stronger evidence of predictive power in this model. In other words, it appeared the existing data lends itself much more strongly to predicting whether an individual will leave academia or quit their job after an incident – the classifier succeeds about 4 out of 5 times.&lt;/p&gt;

&lt;p&gt;From a domain perspective, this finding has a strong potential to tell a compelling story about how sexual harassment and assault incidents drastically alter the lives of targets. Given the moderately strong predictive power of this model, I felt it was appropriate to delve into the specific features and their relative importance. This resulted in what I believe to be some of the strongest findings of the project:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/img/harassment/image6.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Of all possible words that appear within “response” descriptions, the strongest predictors of a “left” or “quit” outcome appear to be a non-response (“none”, blank, “nothing”, “never”) or mention of a supervisor (“boss”). If we were interested in preventing the outcome of targets leaving academia, we might be motivated to investigate these cases of non-responses – what allowed for the absence of any response whatsoever? Was there any good faith intention made to investigate any aspect of these cases?&lt;/p&gt;

&lt;p&gt;Furthermore, how do direct supervisors engage with investigations into sexual harassment and assault, and in what contexts do they fail to follow up at all after a report?&lt;/p&gt;

&lt;p&gt;An incredibly important caveat is that we cannot, from the results above, definitively state the &lt;em&gt;direction&lt;/em&gt; of this association. It’s possible, for example, that a “none” response is correlated with &lt;em&gt;not leaving&lt;/em&gt; academia. What we can say more generally is that, if we know about the way a department or institution responds to sexual harassment/assault, we can develop a pretty good guess of whether the target will eventually leave their graduate program or their academic career. This exploratory finding might give us pause and prompt us to consider how a non-response might be systematically encouraging individuals to abandon their studies and careers.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/img/harassment/image7.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Quite similarly, events that include references to people in direct positions of power (“supervisor”, “dean”) appear predictive of an individual leaving or quitting their position or academic career.&lt;/p&gt;

&lt;p&gt;Supervisors and deans represent two very important archetypes of power within academic institutions: one of direct, immediate responsibility over a student, and one of ultimate power over an entire small departmental community (which, quite often, is separated from the rest of the academic community in significant ways). I wanted to investigate where else “supervisor” or “dean” appeared in the reports, beyond the event field above, and so I entered both terms individually into the search field of &lt;a href=&quot;http://people.csail.mit.edu/karger/Exhibit/Harass/&quot;&gt;the online database of reports&lt;/a&gt;. Unfortunately, but expectedly, the terms frequently appear in the “perpetrator” fields of the reports as well as the event field.&lt;/p&gt;

&lt;p&gt;As in previous sections, we find the lab setting again singled out as significant. I wonder what that might imply about the lab, both as a community space and as physical space. To what degree do the particular affordances of academic labs enable perpetrators to harm their targets? And in this case, what about those patterns of harm make the presence of “lab” predictive of an eventual outcome of the target leaving or quitting academia?&lt;/p&gt;

&lt;p&gt;These two visualizations point to a possible correlation between individuals experiencing assault and harassment from their direct supervisors and eventually leaving or quitting their career/academic pursuits, as well as a relationship between non-responses after incidents and leaving/quitting. This might compel us to direct our attention towards (1) the direct supervision of individuals and (2) peer interactions in departmental spaces such as labs as focal areas for gathering more data about sexual harassment and assault.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;At the outset of this analysis, I posed the question: &lt;strong&gt;what does NLP/machine learning contribute that makes it worth paying attention to when we have the words of targets &amp;amp; survivors in front of us?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Natural language processing and machine learning algorithms are often criticized for promising empirical insights while remaining opaque in how they are producing those results – and that criticism is entirely fair! What I’m curious about is what happens when, rather than leaning on these tools to predict outcomes or manufacture new evidence, we incorporate them into a process of interrogating structures of power along with our own blind-spots.&lt;/p&gt;

&lt;p&gt;What if the outcome of computational text analysis was not a set of algorithmically-divined conclusions so much as a new ethic of investigation? What if instead of waiting for disciplinary boards and committees to find evidence of assault that rises above some satisfactory threshold, we in academic communities start investigating the safeguards (or lack thereof) that protect students from predatory advisors and supervisors? What if we demand focus on the profound structural factors that discourage targets from reporting perpetrators, or the cumulative effect of watching faculty member after faculty member receive no substantial consequence from their actions, or the experiences of individuals who end up leaving their professions and studies entirely?&lt;/p&gt;

&lt;p&gt;The process of exploratory data science I’ve outlined here is, by its nature, deeply iterative. Data leads to analysis, which leads to more carefully gathered and framed data, which leads to new forms of analysis, which leads to yet more data. But while it takes time to develop the skills to get an LDA topic modeling pipeline up and running, it isn’t contingent on a year-long peer review process for a journal publication, or a department’s internal process for responding to complaints against its faculty members. Perhaps this type of investigation may belong within an academic research process, or perhaps it can be utilized by graduate student unions, whistleblowers within academic institutions, investigative media outlets, or whomever wishes to derive practical follow-up actions from complex data.&lt;/p&gt;

&lt;p&gt;My hope is that you find something useful in these questions and observations – whether that means continuing to develop new methods of data collection and analysis to confront sexual harassment in academia, or as an impetus to get excited about critical data science in the context of the questions you care about most deeply. (As for me, I am still finding my voice within this realm of code and communities and data, and plan to continue adding essays, tutorials, and weird little hybrids to this space.)&lt;/p&gt;

&lt;p&gt;Onwards!&lt;/p&gt;

&lt;h2 id=&quot;acknowledgments&quot;&gt;Acknowledgments&lt;/h2&gt;

&lt;p&gt;My sincere thanks to Justin for helping me think through the “critical” component of critical data science in this project and beyond. Thank you also to Diana for suggesting how to making this piece more useful for a wider audience, and to Sarah for generous feedback &amp;amp; enthusiasm for dreamy data science vocab (random forests! seaborn!)&lt;/p&gt;

&lt;h2 id=&quot;notes&quot;&gt;Notes&lt;/h2&gt;

&lt;p&gt;In the original report, I also included a round of analysis countinng ngrams (an additional NLP strategy), which I excluded here in order to focus on the more ML-intensive methods. To read more about those methods and see those findings (which closely mirror the trends re: lack of repercussions and importance of supervision relationships/common spaces like labs/etc.), you may wish to review my original report &lt;a href=&quot;https://github.com/zoews/academic-harassment-data/blob/master/si618wn2018_report_zoews.pdf&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;explore-the-data-and-code-yourself&quot;&gt;Explore the data and code yourself&lt;/h2&gt;

&lt;p&gt;The data (as a .csv file) and code (as a Jupyter notebook) are available at the &lt;a href=&quot;https://github.com/zoews/academic-harassment-data/&quot;&gt;project repository on GitHub&lt;/a&gt;. Feel free to explore further. (I also plan to release a more technical tutorial on building an NLP/ML pipeline in Spark soon!)&lt;/p&gt;
</description>
        <pubDate>Tue, 07 Aug 2018 13:22:00 -0400</pubDate>
        <link>http://localhost:4000/articles/2018-08/sexual-harassment</link>
        <guid isPermaLink="true">http://localhost:4000/articles/2018-08/sexual-harassment</guid>
        
        
        <category>essay</category>
        
      </item>
    
  </channel>
</rss>
